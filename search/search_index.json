{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"cereggii","text":"<p>Thread synchronization utilities for Python</p> <pre><code>pip install cereggii\n</code></pre> <p>This package provides atomic versions of common data structures:</p> <ul> <li>AtomicDict</li> <li>AtomicInt64</li> <li>AtomicRef</li> <li>\u2026and more to come</li> </ul> <p>If you are novel to concurrent programming, you can read the Concurrency 101 guide.</p>"},{"location":"api/AtomicDict/","title":"AtomicDict","text":"<p>A concurrent dictionary (hash table).</p> <p>Features of AtomicDict:</p> <ol> <li> <p>you don't need an external lock to synchronize changes (mutations) to the dictionary:</p> <ol> <li>you don't have to manually guard your code against deadlocks (reentrancy-caused deadlocks can still be an issue)</li> <li>when <code>AtomicDict</code> is correctly configured (setting <code>min_size</code> so that no resizing occurs), even if the OS decides to interrupt or terminate a thread which was accessing an <code>AtomicDict</code>, all remaining threads will continue to make progress</li> </ol> </li> <li> <p>mutations are atomic and can be aborted or retried under contention</p> </li> <li> <p>scalability:</p> <ol> <li>TODO</li> <li>for some workloads scalability is already quite good: see <code>AtomicDict.reduce</code>.</li> </ol> </li> </ol> <p>Note</p> <p>The special <code>cereggii.NOT_FOUND</code>, <code>cereggii.ANY</code>, and <code>cereggii.EXPECTATION_FAILED</code> objects cannot be used as keys nor values.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.__init__","title":"<code>__init__(initial={}, *, min_size=None, buffer_size=4)</code>","text":"<p>Correctly configuring the <code>min_size</code> parameter avoids resizing the <code>AtomicDict</code>. Inserts that spill over this size will not fail, but may require resizing. Resizing prevents concurrent mutations until completed.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>dict</code> <p>A <code>dict</code> to initialize this <code>AtomicDict</code> with.</p> <code>{}</code> <code>min_size</code> <code>int | None</code> <p>The size initially allocated.</p> <code>None</code> <code>buffer_size</code> <code>int</code> <p>The amount of entries that a thread reserves for future insertions. A larger value can help reducing contention, but may lead to increased fragmentation. Min: 1, max: 64.</p> <code>4</code>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Atomically read the value associated with <code>key</code>: <pre><code>my_atomic_dict[key]\n</code></pre></p> <p>Also see <code>get</code>.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Unconditionally set the value associated with <code>key</code> to <code>value</code>: <pre><code>my_atomic_dict[key] = value\n</code></pre></p> <p>Warning</p> <p>Use <code>compare_and_set</code> instead.</p> <p>When an item is inserted or updated with this usual Python idiom, it is not possible to know that the value currently associated with <code>key</code> is the one being expected -- it may be mutated by another thread before this mutation is applied. Use this method only when no other thread may be writing to <code>key</code>.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Atomically delete an item: <pre><code>del my_atomic_dict[key]\n</code></pre></p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.get","title":"<code>get(key, default=None)</code>","text":"<p>Just like Python's <code>dict.get</code>: <pre><code>my_atomic_dict.get(key, default=cereggii.NOT_FOUND)\n</code></pre></p> <p>Tip</p> <p>The special <code>cereggii.NOT_FOUND</code> object can never be inserted into an <code>AtomicDict</code>, so when it is returned, you are ensured that the key was not in the dictionary.</p> <p>Conversely, <code>None</code> can both be a key and a value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>AtomicDict[Key]</code> <p>The key to be looked up.</p> required <code>default</code> <code>AtomicDict[Value] | None</code> <p>The value to return when the key is not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>AtomicDict[Value]</code> <p>The value associated with <code>key</code>, or <code>default</code>.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.compare_and_set","title":"<code>compare_and_set(key, expected, desired)</code>","text":"<p>Atomically read the value associated with <code>key</code>:</p> <ul> <li>if it is <code>expected</code>, then replace it with <code>desired</code></li> <li>else, don't change it and raise <code>ExpectationFailed</code>.</li> </ul> <p>Example</p> <p>Insert only</p> <p>The expected value can be <code>cereggii.NOT_FOUND</code>, in which case the call will succeed only when the item is inserted, and not updated:</p> <pre><code>my_atomic_dict.compare_and_set(\n    key=\"spam\",\n    expected=cereggii.NOT_FOUND,\n    desired=42,\n)\n</code></pre> <p>Example</p> <p>Counter</p> <p>Correct way to increment the value associated with <code>key</code>, coping with concurrent mutations:</p> <pre><code>done = False\nwhile not done:\n    expected = my_atomic_dict.get(key, default=0)\n    try:\n        my_atomic_dict.compare_and_set(key, expected, desired=expected + 1)\n    except cereggii.ExpectationFailed:\n        # d[key] was concurrently mutated: retry\n        pass\n    else:\n        done = True\n</code></pre> <p>The <code>reduce</code> method removes a lot of boilerplate.</p> <p>Tip</p> <p>This family of methods is the recommended way to mutate an <code>AtomicDict</code>. Though, you should probably want to use a higher-level method than <code>compare_and_set</code>, like <code>reduce</code>.</p> <p>Raises:</p> Type Description <code>ExpectationFailed</code> <p>If the found value was not <code>expected</code>.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce","title":"<code>reduce(iterable, aggregate)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>aggregate</code>.</p> <p>The <code>aggregate</code> function takes as input a key, the value currently stored in the dictionary, and the new value from <code>iterator</code>. It returns the aggregated value.</p> <p>Several specialized methods are available to perform common operations:</p> <ul> <li><code>reduce_sum</code></li> <li><code>reduce_and</code></li> <li><code>reduce_or</code></li> <li><code>reduce_max</code></li> <li><code>reduce_min</code></li> <li><code>reduce_list</code></li> <li><code>reduce_count</code></li> </ul> <p>Note</p> <p>The <code>aggregate</code> function must be:</p> <ul> <li>total \u2014 it should handle both the case in which the key is present and in which it is not</li> <li>state-less \u2014 you should not rely on the number of times this function is called (it will be called at least once for each item in <code>iterable</code>, but there is no upper bound)</li> <li>commutative and associative \u2014 the result must not depend on the order of calls to <code>aggregate</code></li> </ul> <p>Example</p> <p>Counter</p> <pre><code>d = AtomicDict()\n\ndata = [\n    (\"red\", 1),\n    (\"green\", 42),\n    (\"blue\", 3),\n    (\"red\", 5),\n]\n\ndef count(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return new\n    return current + new\n\nd.reduce(data, count)\n</code></pre> <p>Info</p> <p>This method exploits the skewness in the data.</p> <p>First, an intermediate result is aggregated into a thread-local dictionary and then applied to the shared <code>AtomicDict</code>. This can greatly reduce contention when the keys in the input are repeated.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_sum","title":"<code>reduce_sum(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>sum()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = value\n    else:\n        atomic_dict[key] += value\n</code></pre> <p>Behaves exactly as if <code>reduce</code> had been called like this:</p> <pre><code>def sum_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return new\n    return current + new\n\nd.reduce(..., sum_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_and","title":"<code>reduce_and(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>all()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = not not value\n    else:\n        atomic_dict[key] = atomic_dict[key] and (not not value)\n</code></pre> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>def and_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return not not new\n    return current and (not not new)\n\nd.reduce(..., and_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_or","title":"<code>reduce_or(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>any()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = not not value\n    else:\n        atomic_dict[key] = atomic_dict[key] or (not not value)\n</code></pre> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>def or_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return not not new\n    return current or (not not new)\n\nd.reduce(..., or_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_max","title":"<code>reduce_max(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>max()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = value\n    else:\n        atomic_dict[key] = max(value, atomic_dict[key])\n</code></pre> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>def max_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return new\n    return max(new, current)\n\nd.reduce(..., max_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_min","title":"<code>reduce_min(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>min()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = value\n    else:\n        atomic_dict[key] = min(value, atomic_dict[key])\n</code></pre> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>def min_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return new\n    return min(new, current)\n\nd.reduce(..., min_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_list","title":"<code>reduce_list(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, as computed by <code>list()</code>.</p> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>def to_list(obj):\n    if type(obj) is list:\n        return obj\n    return [obj]\n\nfor key, value in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = to_list(value)\n    else:\n        atomic_dict[key] = to_list(atomic_dict[key]) + to_list(value)\n</code></pre> <p>Warning</p> <p>The order of the elements in the returned list is undefined. This method will put all the elements from the input in the resulting list: their presence is guaranteed, but the order is not.</p> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>def list_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return to_list(new)\n    return to_list(current) + to_list(new)\n\nd.reduce(..., list_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.reduce_count","title":"<code>reduce_count(iterable)</code>","text":"<p>Aggregate the values in this dictionary with those found in <code>iterable</code>, by counting the number of occurrences of each key. (This is similar to the behavior of <code>collections.Counter</code>.)</p> <p>Note</p> <p>Differently from reduce, this method does not interpret the input as an iterable of key-value pairs, but rather as an iterable of keys.</p> <p>From a dict</p> <p>A <code>dict[Any, int]</code> can also be used, instead of an <code>Iterable[Key]</code>. This follows the behavior of <code>collections.Counter</code>.</p> <pre><code>my_atomic_dict = AtomicDict({\"spam\": 1})\nmy_atomic_dict.reduce_count({\"spam\": 10, \"eggs\": 2, \"ham\": 3})\nassert my_atomic_dict[\"spam\"] == 11\nassert my_atomic_dict[\"eggs\"] == 2\nassert my_atomic_dict[\"ham\"] == 3\n</code></pre> <p>Multiple threads calling this method would effectively parallelize this single-threaded program:</p> <pre><code>for key in iterable:\n    if key not in atomic_dict:\n        atomic_dict[key] = 1\n    else:\n        atomic_dict[key] += 1\n</code></pre> <p>Behaves exactly as if reduce had been called like this:</p> <pre><code>import itertools\n\ndef sum_fn(key, current, new):\n    if current is cereggii.NOT_FOUND:\n        return new\n    return current + new\n\nd.reduce(zip(..., itertools.repeat(1)), sum_fn)\n</code></pre> <p>Tip</p> <p>The implementation of this operation is internally optimized. It is recommended to use this method instead of calling <code>reduce</code> with a custom function.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of items in this <code>AtomicDict</code>: <pre><code>len(my_atomic_dict)\n</code></pre></p> <p>This method is sequentially consistent. When invoked, it temporarily locks the <code>AtomicDict</code> instance to compute the result. If you need to invoke this method frequently while the dictionary is being mutated, consider using <code>AtomicDict.approx_len</code> instead.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.approx_len","title":"<code>approx_len()</code>","text":"<p>Retrieve the approximate length of this <code>AtomicDict</code>.</p> <p>Calling this method does not prevent other threads from mutating the dictionary.</p> <p>Note</p> <p>Differently from <code>AtomicDict.__len__</code>, this method does not return a sequentially consistent result.</p> <p>This is not so bad!</p> <p>Suppose you call <code>AtomicDict.__len__</code> instead. You would get a result that was correct in between the invocation of <code>len()</code>. But, at the very next line, the result might get invalidated by another thread inserting or deleting an item.</p> <p>If you need to know the size of an <code>AtomicDict</code> while other threads are mutating it, calling <code>approx_len</code> should be more performant and still return a fairly good approximation.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.len_bounds","title":"<code>len_bounds()</code>","text":"<p>Get a lower and an upper-bound for the number of items stored in this <code>AtomicDict</code>.</p> <p>Deprecated</p> <p>Use <code>AtomicDict.approx_len</code> instead.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.fast_iter","title":"<code>fast_iter(partitions=1, this_partition=0)</code>","text":"<p>A fast, not sequentially consistent iterator.</p> <p>Calling this method does not prevent other threads from mutating this <code>AtomicDict</code>.</p> <p>Danger</p> <p>This method can return sequentially-inconsistent results.</p> <p>Only use <code>fast_iter</code> when you know that no other thread is mutating this <code>AtomicDict</code>.</p> <p>Depending on the execution, the following may happen:</p> <ul> <li>it can return the same key multiple times (with the same or different values)</li> <li>an update 1, that happened strictly before another update 2, is not seen,   but update 2 is seen.</li> </ul> <p>Tip</p> <p>If no other thread is mutating this <code>AtomicDict</code> while a thread is calling this method, then this method is safe to use.</p> <p>Example</p> <p>Summing an array with partitioning</p> <pre><code>n = 3\npartials = [0 for _ in range(n)]\n\ndef iterator(i):\n    current = 0\n    for k, v in d.fast_iter(partitions=n, this_partition=i):\n        current += v\n    partials[i] = current\n\nthreads = [\n    threading.Thread(target=iterator, args=(i,))\n    for i in range(n)\n]\n\nfor t in threads:\n    t.start()\nfor t in threads:\n    t.join()\n\nprint(sum(partials))\n</code></pre> <p>Also see the partitioned iterations example.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <p>The number of partitions to split this iterator with. It should be equal to the number of threads that participate in the iteration.</p> <code>1</code> <code>this_partition</code> <p>This thread's assigned partition. Valid values are from 0 to <code>partitions</code>-1.</p> <code>0</code>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.batch_getitem","title":"<code>batch_getitem(batch, chunk_size=128)</code>","text":"<p>Batch many lookups together for efficient memory access.</p> <p>The values provided in <code>batch</code> will be substituted with the values found in the <code>AtomicDict</code> instance, or with <code>cereggii.NOT_FOUND</code>. Notice no exception is thrown: the <code>cereggii.NOT_FOUND</code> object instead is the returned value for a key that wasn't present.</p> <p>Notice that the <code>cereggii.NOT_FOUND</code> object can never be inserted into an <code>AtomicDict</code>.</p> <p>The values themselves, provided in <code>batch</code>, will always be substituted.</p> <p>Example</p> <p>With: <pre><code>foo = AtomicDict({'a': 1, 'b': 2, 'c': 3})\n</code></pre></p> <p>calling <pre><code>foo.batch_getitem({\n    'a': None,\n    'b': None,\n    'f': None,\n})\n</code></pre></p> <p>returns: <pre><code>{\n   'a': 1,\n   'b': 2,\n   'f': &lt;cereggii.NOT_FOUND&gt;,\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>subdivide the keys to lookup from <code>batch</code> in smaller chunks of size `chunk_size to prevent memory over-prefetching.</p> <code>128</code> <p>Returns:</p> Type Description <code>dict</code> <p>the input <code>batch</code> dictionary, with substituted values.</p>"},{"location":"api/AtomicDict/#cereggii._cereggii.AtomicDict.get_handle","title":"<code>get_handle()</code>","text":"<p>Get a thread-local handle for this <code>AtomicDict</code>.</p> <p>When using a thread-local handle, you can improve the performance of your application.</p> <p>See <code>ThreadHandle</code> for more information on thread-local object handles.</p>"},{"location":"api/AtomicDict/#cereggii.NOT_FOUND","title":"<code>cereggii.NOT_FOUND</code>  <code>module-attribute</code>","text":"<p>A singleton object. Used in <code>AtomicDict</code> to signal that a key was not found.</p>"},{"location":"api/AtomicDict/#cereggii.ANY","title":"<code>cereggii.ANY</code>  <code>module-attribute</code>","text":"<p>A singleton object. Used in <code>AtomicDict</code> as input for an unconditional update (upsert).</p>"},{"location":"api/AtomicDict/#cereggii.EXPECTATION_FAILED","title":"<code>cereggii.EXPECTATION_FAILED</code>  <code>module-attribute</code>","text":"<p>A singleton object. Used in <code>AtomicDict</code> to return that an operation was aborted due to a failed expectation.</p>"},{"location":"api/AtomicInt64/","title":"AtomicInt64","text":"<p>               Bases: <code>int</code></p> <p>An <code>int</code> that may be updated atomically.</p> <p>Warning</p> <p>AtomicInt64 is bound to 64-bit signed integers: each of its methods may raise <code>OverflowError</code>.</p> <p><code>AtomicInt64</code> borrows part of its API from Java's <code>AtomicInteger</code>, so that it should feel familiar to use, if you're coming to Python from Java. It also implements most numeric magic methods, so that it should feel comfortable to use for Pythonistas.</p> <p>Note</p> <p>The hash of an <code>AtomicInt64</code> is independent of its value. Two <code>AtomicInt64</code>s may have the same hash, but hold different values. They may also have different hashes, but hold the same values.</p> <p>If you need to get the hash of the currently stored <code>int</code> value, you should do this: <pre><code>hash(my_atomic_int.get())\n</code></pre></p> <p>An <code>AtomicInt64</code> and all of its associated <code>AtomicInt64Handle</code>s share the same hash value.</p> <p>Note</p> <p>The following operations are supported by <code>int</code>, but not <code>AtomicInt64</code>:</p> <ul> <li><code>__itruediv__</code> (e.g. <code>my_atomic_int /= 3.14</code> \u2014 an <code>AtomicInt64</code> cannot be used to store floats)</li> <li><code>as_integer_ratio</code></li> <li><code>bit_length</code></li> <li><code>conjugate</code></li> <li><code>from_bytes</code></li> <li><code>to_bytes</code></li> <li><code>denominator</code></li> <li><code>numerator</code></li> <li><code>imag</code></li> <li><code>real</code></li> </ul> <p>You can of course call <code>get</code> on <code>AtomicInt64</code> and then call the desired method on the standard <code>int</code> object.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.__init__","title":"<code>__init__(initial_value=0)</code>","text":""},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.compare_and_set","title":"<code>compare_and_set(expected, desired)</code>","text":"<p>Atomically read the current value of this <code>AtomicInt64</code>:</p> <ul> <li>if it is <code>expected</code>, then replace it with <code>desired</code> and return <code>True</code></li> <li>else, don't change it and return <code>False</code>.</li> </ul>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get","title":"<code>get()</code>","text":"<p>Atomically read the current value of this <code>AtomicInt64</code>.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.set","title":"<code>set(desired)</code>","text":"<p>Unconditionally set the value of this <code>AtomicInt64</code> to <code>desired</code>.</p> <p>Warning</p> <p>Use <code>compare_and_set</code> instead.</p> <p>When using this method, it is not possible to know that the value currently stored is the one being expected -- it may be mutated by another thread before this mutation is applied. Use this method only when no other thread may be writing to this <code>AtomicInt64</code>.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get_and_set","title":"<code>get_and_set(desired)</code>","text":"<p>Atomically swap the value of this <code>AtomicInt64</code> to <code>desired</code> and return the previously stored value.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.increment_and_get","title":"<code>increment_and_get(amount=1)</code>","text":"<p>Atomically increment this <code>AtomicInt64</code> by <code>amount</code> and return the incremented value.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get_and_increment","title":"<code>get_and_increment(amount=1)</code>","text":"<p>Like <code>increment_and_get</code>, but returns the value that was stored before applying this operation.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.decrement_and_get","title":"<code>decrement_and_get(amount=1)</code>","text":"<p>Atomically decrement this <code>AtomicInt64</code> by <code>amount</code> and return the decremented value.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get_and_decrement","title":"<code>get_and_decrement(amount=1)</code>","text":"<p>Like <code>decrement_and_get</code>, but returns the value that was stored before applying this operation.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.update_and_get","title":"<code>update_and_get(callable)</code>","text":"<p>Atomically update the value currently stored in this <code>AtomicInt64</code> by applying <code>callable</code> and return the updated value.</p> <p><code>callable</code> should be a function that takes one <code>int</code> parameter and returns an <code>int</code>.</p> <p>Warning</p> <p>The <code>callable</code> function must be stateless: it will be called at least once but there is no upper bound to the number of times it will be called within one invocation of this method.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get_and_update","title":"<code>get_and_update(callable)</code>","text":"<p>Like <code>update_and_get</code>, but returns the value that was stored before applying this operation.</p>"},{"location":"api/AtomicInt64/#cereggii._cereggii.AtomicInt64.get_handle","title":"<code>get_handle()</code>","text":"<p>Get a thread-local handle for this <code>AtomicInt64</code>.</p> <p>When using a thread-local handle, you can improve the performance of your application.</p> <p>See <code>ThreadHandle</code> for more information on thread-local object handles.</p>"},{"location":"api/AtomicRef/","title":"AtomicRef","text":"<p>An object reference that may be updated atomically.</p>"},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.__init__","title":"<code>__init__(initial_value=None)</code>","text":""},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.compare_and_set","title":"<code>compare_and_set(expected, desired)</code>","text":"<p>Atomically read the current value of this <code>AtomicRef</code>:</p> <ul> <li>if it is <code>expected</code>, then replace it with <code>desired</code> and return <code>True</code></li> <li>else, don't change it and return <code>False</code>.</li> </ul>"},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.get","title":"<code>get()</code>","text":"<p>Atomically read the current value of this <code>AtomicRef</code>.</p>"},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.get_and_set","title":"<code>get_and_set(desired)</code>","text":"<p>Atomically swap the value of this <code>AtomicRef</code> to <code>desired</code> and return the previously stored value.</p>"},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.get_handle","title":"<code>get_handle()</code>","text":"<p>Get a thread-local handle for this <code>AtomicRef</code>.</p> <p>When using a thread-local handle, you can improve the performance of your application.</p> <p>See <code>ThreadHandle</code> for more information on thread-local object handles.</p>"},{"location":"api/AtomicRef/#cereggii._cereggii.AtomicRef.set","title":"<code>set(desired)</code>","text":"<p>Unconditionally set the value of this <code>AtomicRef</code> to <code>desired</code>.</p> <p>Warning</p> <p>Use <code>compare_and_set</code> instead.</p> <p>When using this method, it is not possible to know that the value currently stored is the one being expected -- it may be mutated by another thread before this mutation is applied. Use this method only when no other thread may be writing to this <code>AtomicRef</code>.</p>"},{"location":"api/CountDownLatch/","title":"CountDownLatch","text":"<p>This mimics the <code>java.util.concurrent.CountDownLatch</code> class. Paraphrasing from Java's documentation:</p> <p>Implements a synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.</p> <p>The count starts at a given number, and threads decrement it until the count reaches zero, at which point waiting threads are released and any subsequent invocations of <code>wait()</code> return immediately.</p> <p>This is a one-shot phenomenon \u2014 the count cannot be reset.</p> <p>A CountDownLatch initialized to N can be used to make one thread wait until N threads have completed some action, or some action has been completed N times.</p>"},{"location":"api/CountDownLatch/#cereggii.CountDownLatch.__init__","title":"<code>__init__(count)</code>","text":"<p>Initializes a CountDownLatch with a specified count value.</p> <p>If the initial count is set to zero, all calls to <code>wait()</code> return immediately.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>The number of times <code>decrement()</code> or <code>decrement_and_get()</code> must be called before threads can pass through <code>wait()</code>. Must be a non-negative integer.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the count is less than 0.</p>"},{"location":"api/CountDownLatch/#cereggii.CountDownLatch.wait","title":"<code>wait()</code>","text":"<p>Block the current thread until the count reaches zero, due to invocations of <code>decrement()</code> or <code>decrement_and_get()</code> in other threads.</p>"},{"location":"api/CountDownLatch/#cereggii.CountDownLatch.decrement","title":"<code>decrement()</code>","text":"<p>Decreases the count of the latch by one and wakes up any waiting threads if the count reaches zero.</p> <p>If the current count equals zero, then nothing happens.</p>"},{"location":"api/CountDownLatch/#cereggii.CountDownLatch.decrement_and_get","title":"<code>decrement_and_get()</code>","text":"<p>Like <code>decrement()</code>, but additionally returns the observed count after the decrement.</p>"},{"location":"api/CountDownLatch/#cereggii.CountDownLatch.get","title":"<code>get()</code>","text":"<p>Returns the current count.</p>"},{"location":"api/ThreadHandle/","title":"ThreadHandle","text":"<p>               Bases: <code>ThreadHandle[T]</code></p> <p>A thread-local handle for an object. Acts as a proxy for the handled object. It behaves exactly like the object it handles, and provides some performance benefits.</p> <p>Tip</p> <p>Make sure the thread that created an instance of ThreadHandle is the only thread that uses the handle.</p> <p>Note</p> <p>ThreadHandle is immutable: once instantiated, you cannot change the handled object. If you need a mutable shared reference to an object, take a look at AtomicRef.</p> <p>Warning</p> <p>ThreadHandle does not enforce thread locality. You may be able to share one handle among several threads, but this is not the intended usage.</p> <p>Furthermore, the intended performance gains would be lost if the handle is used by multiple threads.</p> <p>Sharing a ThreadHandle among multiple threads may become unsupported in a future release.</p> <p>How does ThreadHandle improve performance?</p> <p>In free-threading Python, each object has new internal structures that have become necessary for the interpreter to stay correct in the face of multithreaded object use. While they are necessary for correctness, they may slow down an individual thread's access to an object if that object is used by multiple threads. ThreadHandle solves this performance problem by removing the contention on the new object's internal structures.</p> <p>The new structures mentioned are essentially a shared reference counter and a mutex, individually created with each object, in addition to a thread-local reference counter, which was already present in previous versions of Python. This is a very simplified explanation of these changes, to learn more about them, please refer to PEP 703 \u2013 Making the Global Interpreter Lock Optional in CPython.</p> <p>Since ThreadHandle is also an object, it also has its own shared and local reference counters. When a thread makes a call to a method of ThreadHandle, the interpreter implicitly increments the local reference counter and ThreadHandle proxies the call to the handled object. Once the call is completed, the interpreter decrements the local reference counter of ThreadHandle.</p> <p>If ThreadHandle wasn't used and the handled object was used by multiple threads, the reference counting operations would have used the object's shared reference counter. Operations on this counter are atomic and more computationally expensive.</p> <p>If a ThreadHandle is used by multiple threads, then reference counting operations on the handle itself would use the shared reference counter and nullify the performance gains.</p>"},{"location":"api/ThreadSet/","title":"ThreadSet","text":"<p>A container for sets of threads. It provides boilerplate-removing utilities for handling standard library threads. See Python's <code>threading.Thread</code> documentation for general information on Python threads.</p>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.__init__","title":"<code>__init__(*threads)</code>","text":"<p>You can initialize a <code>ThreadSet</code> with any threads you already defined:</p> <pre><code>class MyCustomThreadSubclass(threading.Thread):\n    ...\n\nthreads = ThreadSet(\n    MyCustomThreadSubclass(...),\n    threading.Thread(...),\n)\nthreads.start_and_join()\n</code></pre> <p>You can create unions of <code>ThreadSet</code>s: <pre><code>readers = ThreadSet(...)\nreaders.start()\n\nwriters = ThreadSet(...)\nwriters.start()\n\n(readers | writers).join()\n</code></pre></p> <p>You can create a <code>ThreadSet</code> in a function definition, using <code>with_args()</code>, <code>repeat()</code>, or <code>target</code>: <pre><code>@ThreadSet.repeat(10)  # will create 10 threads\ndef workers():\n    ...\n\nworkers.start_and_join()\n</code></pre></p> <p>With <code>ThreadSet.target</code>, function typing information is kept: <pre><code>@ThreadSet.target\ndef reader(thread_id: int, color: str):\n    ...\n\n@ThreadSet.target\ndef writer(thread_id: int, color: str):\n    ...\n\nthreads = ThreadSet(\n    reader(0, \"red\"),  # your IDE will show the correct types here\n    reader(1, \"green\"),\n    writer(2, \"blue\")\n)\nthreads.start_and_join()\n</code></pre></p>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.with_args","title":"<code>with_args(*args)</code>  <code>classmethod</code>","text":"<pre><code>@ThreadSet.with_args(ThreadSet.Args(1, color=\"red\"), ThreadSet.Args(2, \"blue\"))\ndef spam(thread_id, color):\n    ...\n\nspam.start_and_join()\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.repeat","title":"<code>repeat(times)</code>  <code>classmethod</code>","text":"<pre><code>@ThreadSet.repeat(5)\ndef workers():\n    ...\n\nworkers.start_and_join()\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.target","title":"<code>target(target)</code>  <code>classmethod</code>","text":"<pre><code>@ThreadSet.target\ndef spam(thread_id: int, color: str):\n    ...\n\nthreads = ThreadSet(\n    spam(1, color=\"red\"),\n    spam(2, \"blue\"),\n)\nthreads.start_and_join()\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.start","title":"<code>start()</code>","text":"<p>Start the threads in this <code>ThreadSet</code>.</p> <p>Also see <code>Thread.start()</code>.</p>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.join","title":"<code>join(timeout=None)</code>","text":"<p>Join the threads in this <code>ThreadSet</code>.</p> <p>Also see <code>Thread.join()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>The timeout for each individual join to complete.</p> <code>None</code>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.start_and_join","title":"<code>start_and_join(join_timeout=None)</code>","text":"<p>Start the threads in this <code>ThreadSet</code>, then join them.</p>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.is_alive","title":"<code>is_alive()</code>","text":"<p>Call <code>Thread.is_alive()</code> for each thread in this <code>ThreadSet</code>.</p>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.any_is_alive","title":"<code>any_is_alive()</code>","text":"<pre><code>any(self.is_alive())\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.all_are_alive","title":"<code>all_are_alive()</code>","text":"<pre><code>all(self.is_alive())\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.all_are_not_alive","title":"<code>all_are_not_alive()</code>","text":"<pre><code>not self.any_is_alive()\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.__or__","title":"<code>__or__(other)</code>","text":"<p>Returns a new <code>ThreadSet</code> containing all the threads in the operands, which must be <code>ThreadSet</code> instances.</p> <pre><code>threads = ThreadSet(...) | ThreadSet(...)\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.__ior__","title":"<code>__ior__(other)</code>","text":"<p>Adds the threads in the right operand into this <code>ThreadSet</code>. The right operand must be a <code>ThreadSet</code>.</p> <pre><code>threads = ThreadSet(...)\nthreads |= ThreadSet(...)\n</code></pre>"},{"location":"api/ThreadSet/#cereggii.ThreadSet.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of threads contained in this <code>ThreadSet</code>.</p> <pre><code>len(ThreadSet(...))\n</code></pre>"},{"location":"concurrency-101/","title":"Concurrency 101","text":"<p>Note</p> <p>This aims to be a very practical short guide on concurrent programming. Most theoretical concepts are missing, so it should not be considered a substitute for a full course on Concurrency.</p>"},{"location":"concurrency-101/low-safety/","title":"Low-Level Safety","text":"<p>In this article, we take a bird's eye view over low-level shared-memory safety.</p> <p>Note</p> <p>The information provided here is very abstract. If you're just looking to make your code safe, you should probably use something concrete instead.</p>"},{"location":"concurrency-101/low-safety/#how-to-make-a-multiprocessor-computer-that-correctly-executes-multithreaded-programs","title":"How to make a multiprocessor computer that correctly executes multithreaded programs","text":"<p>In case you ever wandered how to make such computers, the answer is quite straightforward. There are two requirements:</p> <ol> <li>each processor issues memory requests in the order specified by its program</li> <li>memory requests from all processors issued to an individual memory module are    serviced from a single FIFO queue (issuing a memory request consists of entering    the request on this queue)</li> </ol> <p>With these in mind, you should be convinced that the following pseudocode implements a mutual exclusion algorithm for two threads (assume <code>a</code> and <code>b</code> were initialized with 0):</p> Thread AThread B <pre><code>a \u2190 1\nif (b = 0) {print \"A\"}\n</code></pre> <pre><code>b \u2190 1\nif (a = 0) {print \"B\"}\n</code></pre> <p>It should be easy to see that either:</p> <ul> <li>A gets into the critical section (the print instruction),</li> <li>B does, or</li> <li>none of them do.</li> </ul> <p>(In a realistic program you'd wish to make both A and B enter their critical sections eventually, but let's simplify things here.)</p> <p>Now, let's consider the same program written in C:</p> Thread AThread B <pre><code>a = 1;\nif (b == 0) {printf(\"A\\n\");}\n</code></pre> <pre><code>b = 1;\nif (a == 0) {printf(\"B\\n\")}\n</code></pre> <p>This code does not implement mutual exclusion. It is entirely possible to observe this output: <pre><code>B\nA\n</code></pre></p> <p>The unfortunate reality is that both of the above requirements don't hold in general:</p> <ol> <li>there exist memory models (see below)</li> <li>to enhance performance, a processor usually has a so-called store-buffer in    which several store instructions can be held before actually flushing them to    the memory module</li> </ol> <p>This second issue can be solved using memory barriers (also termed fences), or atomic hardware instructions (see section Compare and Swap below).</p>"},{"location":"concurrency-101/low-safety/#memory-models","title":"Memory Models","text":"<p>Note</p> <p>Python does not have a memory model (yet).</p> <p>In a language like C, or Java, when you write a program you are in fact producing two: one is written by you, and the other by your compiler. Those two programs can in fact be very different from each other.</p> <p>In the example above, the C code can produce an executable program in which the load instruction in the second line is issued before the store instruction in the first. Even if we assumed the absence of store-buffers, we may be surprised to still see this behavior:</p> <pre><code>sequenceDiagram\n  autonumber\n  A-&gt;&gt;memory: load b\n  memory-&gt;&gt;A: b = 0\n  B-&gt;&gt;memory: load a\n  memory-&gt;&gt;B: a = 0\n  A-&gt;&gt;memory: store a = 1\n  memory-&gt;&gt;A: OK\n  B-&gt;&gt;memory: store b = 1\n  memory-&gt;&gt;B: OK\n  Note left of A: enter the critical section\n  Note right of B: enter the critical section</code></pre> <p>The compiler has modified the order of our memory operations in a way which has broken the mutual exclusion property of our program.</p> <p>If we were to correct the program and tell the compiler which instructions not to reorder, we would do it like this (assuming our compiler has <code>stdatomic.h</code>):</p> Thread AThread B <pre><code>atomic_store_explicit(&amp;a, 1, memory_order_release);\nif (atomic_load_explicit(&amp;b, memory_order_acquire) == 0) {printf(\"A\\n\");}\n</code></pre> <pre><code>atomic_store_explicit(&amp;b, 1, memory_order_release);\nif (atomic_load_explicit(&amp;a, memory_order_acquire) == 0) {printf(\"B\\n\");}\n</code></pre> <p>For more details on the C memory model, please refer to cppreference.com.</p>"},{"location":"concurrency-101/low-safety/#compare-and-set","title":"Compare and Set","text":"<p>To solve the issue of the store-buffer we may flood our code with memory barriers, but there are more sophisticate instructions that we can use. They are usually called atomic writes/stores/instructions. The most commonly used one is called \"compare and set\" (also termed \"compare and swap\" or \"compare exchange\").</p> <p>The cost of a memory barrier and that of an atomic instruction are roughly the same on most architectures.</p> <p>An atomic compare-and-set instruction is a hardware instruction that takes three arguments: a pointer to a memory location, an expected value, and a desired value. It commonly behaves like this:</p> <ul> <li>read the value contained at memory address M</li> <li>if the value is the expected one:<ul> <li>change stored value to the desired one</li> <li>signal a success (usually setting a register)</li> </ul> </li> <li>otherwise:<ul> <li>don't change the stored value</li> <li>signal a failure.</li> </ul> </li> </ul> <p>This behavior is exactly what's needed to implement any correct concurrent program. No more, no less. You can find a proof of this in M. Herlihy, \"Wait-Free Synchronization\". You can also find a proof as to why normal reads and writes are not enough to implement concurrent programs (have a consensus number of 1).</p> <p>If your compiler has support for <code>stdatomic.h</code>, this gets converted into your architecture's compare-and-set instruction:</p> <pre><code>atomic_compare_exchange_strong_explicit(\n    addr,\n    &amp;expected, desired,\n    memory_order_acq_rel, memory_order_acquire\n);\n</code></pre> <p>Tip</p> <p>With cereggii, this functionality is exposed to Python code:</p> <pre><code>import cereggii\nref = cereggii.AtomicRef(0)\nref.compare_and_set(expected=0, desired=1)\n</code></pre>"},{"location":"concurrency-101/low-safety/#the-implementation-of-cas-false-sharing","title":"The Implementation of CAS \u2014 False Sharing","text":"<p>The implementation of CAS does not affect the correctness of a multithreaded program, but it can greatly impact its performance.</p> <p>See the Performance article for more information on this and on how to get more performance from your shared-memory program.</p>"},{"location":"concurrency-101/low-safety/#the-aba-problem","title":"The ABA Problem","text":"<p>There is a common problem with compare-and-set instructions, known as the ABA problem, that generally pertains lock-free algorithms.</p> <p>Consider this situation:</p> <ol> <li>thread \\(T_1\\) reads memory location \\(M\\), observing value \\(V\\)</li> <li>another thread \\(T_2\\) writes value \\(V'\\) into \\(M\\)</li> <li>\\(T_2\\) writes \\(V\\) into \\(M\\)</li> <li>\\(T_1\\) successfully CASes \\(M\\) from \\(V\\) to another value \\(V''\\)</li> </ol> <p>Was it ok for \\(T_1\\) to have had a success? Maybe not.</p> <p>See Wikipedia's article for more information.</p>"},{"location":"concurrency-101/low-safety/#load-linked-store-conditional","title":"Load-Linked Store-Conditional","text":"<p>Some architectures, notably ARM, implement the compare-and-set instruction described above, with a variant known as Load-Linked/Store-Conditional (LL/SC).</p> <p>Instead of implementing CAS with a single indivisible instruction, these architectures use two instructions. With the first, some cache line is read by one processor and is generally marked as exclusive in cooperation with the cache-coherency protocol. When doing so, the core knows that:</p> <ol> <li>it has the most recent value for that line, and</li> <li>no other processor has the same line marked as exclusive in its own cache.</li> </ol> <p>Later, the store is performed so that it only succeeds in its write if the line is still marked as exclusive. (There are restrictions on how much later this is done, for instance requiring that the load is immediately followed by the store.)</p> <p>It follows that the ABA problem cannot arise: when one core succeeds in its store, it knows that no other core wrote into the same location.</p> <p>The downside is that the CAS operation may fail spuriously when other cores are writing into the same line, even at different locations.</p>"},{"location":"concurrency-101/performance/","title":"Performance","text":""},{"location":"concurrency-101/performance/#its-all-about-the-cache","title":"It's all about the cache","text":"<p>Understanding the performance of a concurrent program is very different from that of a sequential program. Let's turn to C here: the relevant subtleties are much more easily appreciated in a lower-level language.</p> <p>Note</p> <p>Here, we'll use <code>pthreads.h</code> and <code>stdatomic.h</code>. If you're unfamiliar with them, you should still be able to follow along. If you want to also execute this code, be sure to check whether you have these available for your platform, or rewrite the programs with what's available for you. These two libraries are generally available in recent Linux environments.</p> <p>Let's compare these simple C programs, where two threads execute the <code>thread_routine</code> function in parallel:</p> SlowFast <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;stdatomic.h&gt;\n\n\nvoid *thread_routine(void *spam) {\n    int8_t expected = 0, desired = 0;\n\n    for (int i = 0; i &lt; 100000000UL; i++) {\n        atomic_compare_exchange_strong_explicit(\n            (int8_t *) spam, &amp;expected, desired,\n            memory_order_acq_rel, memory_order_acquire // (1)\n        );\n    }\n\n    return spam;\n}\n\nint main(void) {\n    pthread_t thread1, thread2;\n    int8_t* spam = malloc(1);\n\n    int t1 = pthread_create(&amp;thread1, NULL, thread_routine, (void *) spam);\n    int t2 = pthread_create(&amp;thread2, NULL, thread_routine, (void *) spam);\n\n    pthread_join(thread1, NULL);\n    pthread_join(thread2, NULL);\n\n    return t1 + t2;\n}\n</code></pre> <ol> <li>These <code>memory_order_*</code> shenanigans are due to C's memory model. You don't    need to know exactly what they mean to follow this section. If you're curious    you can refer to this.</li> </ol> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;stdatomic.h&gt;\n\n\nvoid *thread_routine(void *spam) {\n    int8_t expected = 0, desired = 0;\n\n    for (int i = 0; i &lt; 100000000UL; i++) {\n        atomic_compare_exchange_strong_explicit(\n            (int8_t *) spam, &amp;expected, desired,\n            memory_order_acq_rel, memory_order_acquire\n        );\n    }\n\n    return spam;\n}\n\nint main(void) {\n    pthread_t thread1, thread2;\n    int8_t* spam = malloc(64 + 1);  // (1)\n\n    int t1 = pthread_create(&amp;thread1, NULL, thread_routine, (void *) spam);\n    int t2 = pthread_create(&amp;thread2, NULL, thread_routine, (void *) (spam + 64));\n\n    pthread_join(thread1, NULL);\n    pthread_join(thread2, NULL);\n\n    return t1 + t2;\n}\n</code></pre> <ol> <li>Allocate the array in two distinct cache lines.</li> </ol> <p>You can easily see that the two programs are executing the same instructions. The only difference is the address the threads execute their routine on. If this was a sequential program, you would expect them to have the same running time. Instead, the \"Slow\" program takes ~2.5s, while the \"Fast\" program takes ~0.5s.</p> <p>Why?</p>"},{"location":"concurrency-101/performance/#the-implementation-of-cas","title":"The Implementation of CAS","text":"<p>A compare-and-set instruction (see here for an introduction) is usually implemented by hardware in cooperation with the cache sub-system. (Other atomic instructions are implemented similarly.)</p> <p>Info</p> <p>The hardware cache is usually divided into pieces of 64 or 128 bytes called lines. When a memory address is read from main memory into the cache, an entire line is loaded into the cache of the core that requested it.</p> <p>It is important to note that in recent architectures the cache itself is a hierarchy of smaller caches, usually divided into L1, L2, and L3 layers. L1 is the \"closest\" to the core, it is the fastest and the smallest. L3 is the \"farthest\" from the core, it is the slowest and the largest.</p> <p>An L1 layer of cache is usually assigned specifically to one core, an L2 layer to a subset of cores, and an L3 to a larger subset still.</p> <p>Therefore, not all cores have access to the same memory. For concurrent shared-memory programs, it means that several cores may refer to the same logical locations of memory from different physical locations.</p> <p>The CPU itself is assigned the task of keeping all the pieces of cache coherent with one another. When several cores share a line of cache and one core modifies it, the hardware has to make sure that the remaining cores eventually see the modification. (Also see Wikipedia's article on cache coherence.)</p> <p>In such a scenario, when one core modifies a shared cache line with an atomic instruction, before it succeeds in its modification it has to ensure that it owns the most recent view of that cache line. When it's certain of that, the check against the expected value is performed.</p> <p>Later, if other cores want to also modify the same line with an atomic instruction, they need to make sure to have the most recent view, too.</p> <p>This behavior can generate a lot of traffic in the memory bus of the CPU, when a line of cache is contended among cores.</p>"},{"location":"concurrency-101/performance/#false-sharing","title":"False Sharing","text":"<p>A corollary to the implementation of CAS is known as False Sharing. </p> <p>When contention occurs, one would expect it only to pertain to a single memory location, but it's not so. It pertains to the entire cache line being contended.</p> <p>Thus, distinct pieces of memory that happen to reside in the same line of cache are falsely shared (and contended), between the CPU cores, triggering traffic in the memory bus.</p>"},{"location":"concurrency-101/performance/#five-rules-of-thumb","title":"Five rules of thumb","text":"<p>There are five practical rules that can help you improve the performance of your program. These are drawn from the excellent book \"The Art of Multiprocessor Programming\" by M. Herlihy and N. Shavit.</p> <ul> <li>Objects or fields that are accessed independently should be aligned and     padded so that they end up on different cache lines.</li> </ul> <p>This is the simplest solution to false sharing: pad your data structures, use more memory, and ensure that objects end up on different lines of cache.</p> <p>You can see an application of this rule in the \"Fast\" variant of the program above.</p> <p>Tip</p> <p>Often a line of cache is 64 bytes wide. You can check the line size in your system by running <code>getconf LEVEL1_DCACHE_LINESIZE</code>.</p> <ul> <li>Keep read-only data separate from data that is modified frequently.</li> </ul> <p>For example, consider a list whose structure is constant, but whose elements' value     fields change frequently. To ensure that modifications do not slow down list     traversals, one could align and pad the value fields so that each one fills up a     cache line.</p> <p>The example is exactly how a Python list works: one array of pointers to <code>PyObject</code>s that reside in other locations.</p> <ul> <li>When possible, split an object into thread-local pieces.</li> </ul> <p>For example, a     counter used for statistics could be split up into an array of counters, one per     thread, each one residing on a different cache line. While a shared counter     would cause invalidation traffic, the split counter allows each thread to update     its own replica without causing coherence traffic.</p> <p>TODO</p> <ul> <li>If a lock protects data that is frequently modified, then keep the lock and     the data on distinct cache lines, so that threads trying to acquire the lock do     not interfere with the lock holder's access to the data.</li> </ul> <p>TODO</p> <ul> <li>If a lock protects data that is frequently uncontended, then try to keep the     lock and the data on the same cache lines, so that acquiring the lock will also     load some of the data into the cache.</li> </ul>"},{"location":"concurrency-101/progress-guarantees/","title":"Progress Guarantees","text":""},{"location":"concurrency-101/progress-guarantees/#issues-of-mutual-exclusion","title":"Issues of Mutual Exclusion","text":""},{"location":"concurrency-101/progress-guarantees/#wait-freedom","title":"Wait-Freedom","text":""},{"location":"concurrency-101/progress-guarantees/#lock-freedom","title":"Lock-Freedom","text":""},{"location":"concurrency-101/safety/","title":"Safety","text":"<p>When multiple threads can mutate (read and write to) the same piece of memory, they need to be synchronized to avoid undefined behavior.</p> <p>https://xkcd.com/2200/</p>"},{"location":"concurrency-101/safety/#mutual-exclusion","title":"Mutual Exclusion","text":""},{"location":"concurrency-101/safety/#reentrancy","title":"Reentrancy","text":""},{"location":"concurrency-101/pymutex/","title":"Recreating PyMutex","text":"<p>In this chapter, we'll try to recreate Sam Gross' PyMutex lock, using the utilities that cereggii provides. This particular lock is very performant, and understanding where the performance is gained will help you understand how to gain performance in your own programs.</p> <p>We'll be writing our PyMutex in Python code, but what you'll see here mostly applies to C as well. In C, you'll have to also take care of correctly following the constraints of the memory model; in Python we don't have to.</p> <p>This is a multipart journey, we'll begin with reviewing the performance of two standard locks, and then we'll delve into the details, choices, and tradeoffs of PyMutex.</p>"},{"location":"concurrency-101/pymutex/#a-simple-correct-lock","title":"A simple, correct lock","text":""},{"location":"concurrency-101/pymutex/#the-next-steps","title":"The next steps","text":"<ul> <li>Part 1 \u2014 TAS vs TTAS</li> <li>...</li> </ul>"},{"location":"concurrency-101/pymutex/test-and-set/","title":"Simple Locks: TAS vs TTAS","text":""},{"location":"examples/AtomicDict/reduce-average/","title":"Using <code>reduce()</code> for Averaging","text":"<p>This example demonstrates a more advanced use case of <code>AtomicDict.reduce()</code> to aggregate key-value pairs across multiple threads. We'll compare the performance of the built-in Python <code>dict</code> with <code>AtomicDict</code> in both single-threaded and multithreaded scenarios.</p> <p>This example has a similar structure to the Using <code>reduce()</code> for Multithreaded Aggregation example. If you haven't read that one already, please consider reading it before continuing.</p> <p>Differently from the other example whose goal was to perform multithreaded summation, here the goal is multithreaded averaging. This is a more challenging problem because of a limitation of <code>reduce()</code>.</p> <p>The interesting part of this example is how we can turn a function that would not be supported by the semantics of <code>reduce()</code>, into one that does.</p>"},{"location":"examples/AtomicDict/reduce-average/#source-code","title":"Source Code","text":"<p>The source code is available on GitHub.</p>"},{"location":"examples/AtomicDict/reduce-average/#generating-data","title":"Generating Data","text":"<p>Like in the previous example, we generate some synthetic data that makes it easy  to check the result. The expected output should always be <code>2.00</code>, plus or minus floats imprecision.</p> <pre><code>size = 3628800\nvalues = [1, 2, 3]\nexpected_avg = sum(values) / len(values)\nprint(f\"{expected_avg=:.2f}\")\n\n\ndef make_keys():\n    return list(range(10))\n\n\ndef make_data(data_size):\n    keys = make_keys()\n    rand = random.Random()\n    v = ThreadHandle(values)\n    return [(keys[_ % len(keys)], rand.choice(v)) for _ in range(data_size)]\n</code></pre>"},{"location":"examples/AtomicDict/reduce-average/#implicit-contention","title":"Implicit Contention","text":"<p>There are two peculiar things we should mention here:</p> <ol> <li>we don't use <code>random.choice</code>, but we instantiate a <code>Random()</code> object of which we call <code>choice()</code>.</li> <li>we wrap the values list with a mysterious <code>ThreadHandle()</code> \ud83d\ude36\u200d\ud83c\udf2b\ufe0f</li> </ol> <p>As in the previous example, the <code>make_data()</code> function is called by every thread that we start. It can be surprising to see that there if we didn't involve the <code>Random()</code> instance and the <code>ThreadHandle()</code> we would have seen severely degraded performance later on.</p> <p>The reason is that we would be accessing two global objects when calling <code>random.choice()</code>  (the module function) and <code>choice(values)</code>. In free-threading Python, when objects are shared between threads, internal data structures involving those objects are contented. If we want good multithreading performance, we must strive to avoid sharing objects.</p> <p>Threads would access those objects in a tight loop. We need to take care of that contention:</p> <ol> <li>instead of using the module-global <code>Random()</code> instance, we create a new one for    each thread; and</li> <li>instead of accessing the <code>values</code> list directly, we wrap it in a <code>ThreadHandle()</code></li> </ol> <p>The <code>ThreadHandle()</code> provided by cereggii is an object that mediates calls to a thread-shared object. This proxying of calls through the thread-local handle avoids implicit shared object contention. To learn more about this kind of contention, please see <code>ThreadHandle()</code>'s documentation.</p> <p>These scenarios can be commonly encountered when dealing with free-threading performance hits.</p> <p>If you edit the example code and remove either one of these mitigations, you'll  see performance dropping.</p>"},{"location":"examples/AtomicDict/reduce-average/#back-with-averaging","title":"Back with Averaging","text":"<p>Now let's get back to why averaging is such a painful multithreading problem. The core of the issue is in the division operation. We cannot apply a division without regard for the order in which threads will apply it.</p> <p>The other issue with averaging is that it requires a total view of the dataset \u2014 we can't incrementally compute the average.</p> <p>These two restrictions are against the requirement of <code>reduce()</code> that </p> <p>The <code>aggregate</code> function must be:</p> <ul> <li>[...]</li> <li>commutative and associative \u2014 the result must not depend on the order of calls to <code>aggregate</code></li> </ul> <p>We simply cannot use <code>reduce()</code> to compute the average of our dataset. But we can do the next-best thing and turn our non-commutative and non-associative function into one that is!</p> <p>Instead of computing the average with our function, we'll compute the sum of the values and keep a count of the items in the input. The division itself will then be delayed until all threads are done.</p> <p>Turning some problem into a summation problem is a good approach for making it  multithreading-friendly.</p>"},{"location":"examples/AtomicDict/reduce-average/#friends-again","title":"Friends Again","text":"<p>Without further ado, let's see the custom <code>aggregate</code> function for <code>reduce()</code>:</p> <pre><code>    def my_reduce_avg(key, current, new) -&gt; tuple[int, int]:\n        if type(new) is not tuple:\n            new = (new, 1)\n        if current is NOT_FOUND:\n            return new\n        current_total, current_count = current\n        new_total, new_count = new\n        return current_total + new_total, current_count + new_count\n</code></pre> <p>This is not as simple as the function we've seen in the previous example. Since we essentially want to keep two counters, it's fair enough that the output of the function is <code>tuple[int, int]</code>.</p> <p>The next line might be more surprising. What is the type of <code>new</code>? It's either:</p> <ul> <li><code>int</code> \u2014 we're reading from the input data; or</li> <li><code>tuple[int, int]</code> \u2014 we're reading a value stored in the dictionary by another thread.</li> </ul> <p>This is a reminder that we need to help <code>reduce()</code> by covering all possible cases of our multithreaded scenario.</p> <p>The rest should be pretty straightforward if you followed the previous example on <code>reduce()</code>.</p>"},{"location":"examples/AtomicDict/reduce-average/#dividing","title":"Dividing","text":"<p>Now that we computed the two sums in a multithreading-friendly way, it's a good time to look at the function that starts the threads. It's been slightly modified from the previous example:</p> <pre><code>def threaded_avg(threads_num, thread_target):\n    atomic_dict = AtomicDict()\n    data_size = size // threads_num\n\n    threads = [\n        threading.Thread(target=thread_target, args=(atomic_dict, data_size))\n        for _ in range(threads_num)\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    for k, (tot, cnt) in atomic_dict.fast_iter():\n        atomic_dict[k] = tot / cnt\n\n    total = 0\n    count = 0\n    for k, avg in atomic_dict.fast_iter():\n        total += avg\n        count += 1\n    return total / count\n</code></pre> <p>The ending of the function is where we do the division. For simplifying result checks, we average again the values pertaining to different keys, but we could've also kept them separate in order to check them separately.</p> <p>The thing I want to point out here is that there is one constraint for where in this code we can or cannot compute the division. Namely, after calling <code>join()</code> on every thread.</p> <p>If we did that after the call to <code>start()</code> and before the call to <code>join()</code> our division here would be racing against the threads running <code>reduce()</code>.</p>"},{"location":"examples/AtomicDict/reduce-average/#single-threaded-baseline","title":"Single-Threaded Baseline","text":"<p>The single-threaded baseline is shown below. I want to encourage you to ponder on this: is the multithreaded counterpart based on <code>reduce()</code> a lot harder to grasp? (Keep in mind that averaging the items in the dictionary at the end is just for the sake of simplicity in checking the result.)</p> <p>You might contend that I've kept it intentionally complicated, but I disagree.</p> <pre><code>def builtin_dict_avg():\n    d = {}\n    keys = make_keys()\n    for k in keys:\n        d[k] = (0, 0)\n    for k, v in make_data(size):\n        tot, cnt = d[k]\n        d[k] = (tot + v, cnt + 1)\n\n    for k, (tot, cnt) in d.items():\n        d[k] = tot / cnt\n\n    total = 0\n    count = 0\n    for k, avg in d.items():\n        total += avg\n        count += 1\n    return total / count\n</code></pre>"},{"location":"examples/AtomicDict/reduce-average/#results","title":"Results","text":"<p>The <code>reduce()</code> function and the constraint of executing the division strictly after the threads are done, provided us with correctness in our computation. Here too, splitting the dataset among several threads helped with performance, but we must recall what we learned in the Implicit Contention section to avoid throwing it all away.</p> <p>The results shown here have been run with a beta version of free-threading Python 3.14.</p> <pre><code>$ python -VV\nPython 3.14.0b2 experimental free-threading build (main, Jun 13 2025, 18:57:59) [Clang 17.0.0 (clang-1700.0.13.3)]\n</code></pre> <p>Note</p> <p>Speedup is relative to the single-threaded baseline. A value below 1.0x means it performed worse than the baseline.</p> <pre><code>expected_avg=2.00\n\nAveraging using the built-in dict with a single thread:\n - Took 0.831s (average=2.00)\n\nAveraging using cereggii.AtomicDict.reduce():\n - Took 0.965s with 1 threads (0.9x faster, average=2.00)\n - Took 0.497s with 2 threads (1.7x faster, average=2.00)\n - Took 0.346s with 3 threads (2.4x faster, average=2.00)\n - Took 0.251s with 4 threads (3.3x faster, average=2.00)\n</code></pre>"},{"location":"examples/AtomicDict/reduce/","title":"Using <code>reduce()</code> for Multithreaded Aggregation","text":"<p>This example demonstrates how to use <code>AtomicDict.reduce()</code> to aggregate key-value pairs across multiple threads. We'll compare the performance of the built-in Python <code>dict</code> with <code>AtomicDict</code> in  both single-threaded and multithreaded scenarios.</p> <p>The example consists of:</p> <ul> <li>synthetic dataset generation</li> <li>baseline single-threaded dictionary summation using <code>dict</code></li> <li>multithreaded implementation using <code>AtomicDict.reduce()</code>   and <code>reduce_sum()</code></li> <li>a performance comparison between the approaches</li> </ul>"},{"location":"examples/AtomicDict/reduce/#source-code","title":"Source Code","text":"<p>The source code is available on GitHub. We'll briefly go through its structure.</p>"},{"location":"examples/AtomicDict/reduce/#generating-data","title":"Generating Data","text":"<p>First, we have some functions generating synthetic data. Essentially, we repeat keys 1\u201310 <code>size</code> times. We use 5 as the value for each item in the data so that we have a predictable total which we can later use to check the outputs.</p> <pre><code>size = 3628800\nexpected_total = size * 5\nprint(f\"{expected_total=}\")\n\n\ndef make_keys():\n    return list(range(10))\n\n\ndef make_data(data_size):\n    keys = make_keys()\n    return [(keys[_ % len(keys)], 5) for _ in range(data_size)]\n</code></pre>"},{"location":"examples/AtomicDict/reduce/#single-threaded-baseline","title":"Single-threaded Baseline","text":"<p>Next, we introduce a single-threaded function that uses Python's built-in <code>dict</code>. This will be useful to compare our multithreaded implementation against a baseline. This function is quite straightforward: make sure the keys are present in the dictionary with an initial value of 0, proceed with summing the values in the input, and then return the sum of the values in the dictionary.</p> <pre><code>def builtin_dict_sum():\n    d = {}\n    keys = make_keys()\n    for k in keys:\n        d[k] = 0\n    for k, v in make_data(size):\n        d[k] += v\n    return sum(d.values())\n</code></pre>"},{"location":"examples/AtomicDict/reduce/#multithreaded-implementation","title":"Multithreaded Implementation","text":"<p>Now let's go through the multithreaded equivalent of <code>builtin_dict_sum</code>. This is what's going on in the function below:</p> <ol> <li>create a new <code>AtomicDict()</code></li> <li>split the dataset into equally sized partitions</li> <li>instantiate and start threads</li> <li>wait for threads to finish</li> <li>sum the values in the dictionary</li> </ol> <pre><code>def threaded_sum(threads_num, thread_target):\n    atomic_dict = AtomicDict()\n    data_size = size // threads_num\n\n    threads = [\n        threading.Thread(target=thread_target, args=(atomic_dict, data_size))\n        for _ in range(threads_num)\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    threaded_total = 0\n    for _, v in atomic_dict.fast_iter():\n        threaded_total += v\n    return threaded_total\n</code></pre> <p>Note</p> <p>The variable <code>size</code> is intentionally set to 10! = 3628800 so that partitioning works cleanly with any number of threads between 1 and 10. A more serious program that doesn't use synthetic data should look for a better way to partition it.</p> <p>The target function that the threads actually run has been intentionally left out  of the snippet above. This is because we'll compare two variants of that: one uses a custom input function for <code>reduce()</code> (the <code>aggregate</code> argument), while the other uses a specialized reduce function that sums the values in the input data, exactly as the custom variant does. As we'll see later, this results not only in more convenience but also better performance.</p>"},{"location":"examples/AtomicDict/reduce/#understanding-reduce-using-a-custom-function","title":"Understanding <code>reduce()</code> \u2014 Using a Custom Function","text":"<p><code>AtomicDict.reduce()</code> applies a user-defined function to every key-value pair in a thread-safe way. This function, such as <code>my_reduce_sum()</code> below, receives:</p> <ul> <li><code>key</code>: the key (first item) of the pair</li> <li><code>current</code>: the current value in the dictionary (or <code>NOT_FOUND</code> if absent)</li> <li><code>new</code>: the new value to aggregate</li> </ul> <p>Note that <code>NOT_FOUND</code> is a special object that cannot be used as either key nor value in AtomicDict. When <code>current is NOT_FOUND</code>, it means that <code>key</code> is not present in the dictionary and so, in this case, it makes sense to return the <code>new</code> value.</p> <pre><code>def thread(atomic_dict, iterations):\n    data = make_data(iterations)\n\n    def my_reduce_sum(key, current, new):\n        if current is NOT_FOUND:\n            return new\n        return current + new\n\n    atomic_dict.reduce(data, my_reduce_sum)\n</code></pre> <p>This is somewhat more compact than the single-threaded version based on <code>dict</code>. The <code>reduce()</code> method offloads from us the reads and writes to the shared dictionary so that we don't inadvertently introduce data races.</p> <p>Since the dictionary may be shared with multiple threads, the current thread executing <code>reduce()</code> may try to update an item and fail because another thread has updated the same item. To cope with this contention, <code>reduce()</code> will call <code>my_reduce_sum()</code> again, and <code>current</code> will hold the value that the other thread had put into the shared dictionary.</p> <p>Tip</p> <p>Do not assume a fixed number of calls to your function. It will be called at least once per input item, but potentially more.</p>"},{"location":"examples/AtomicDict/reduce/#limitations-of-reduce","title":"Limitations of Reduce","text":"<p>Because of this implicit retry mechanism, <code>reduce()</code> imposes some limitations to the <code>aggregate</code> input functions. Most importantly, those functions need to be commutative and associative: the final result must not depend on the order of application.</p> <p>More on how to cope with this limitation in the Using <code>reduce()</code> for Averaging example.</p>"},{"location":"examples/AtomicDict/reduce/#specialized-function","title":"Specialized Function","text":"<p>Summation is a simple and common way of using <code>reduce()</code>. Since it is so, a specialized version is available in <code>AtomicDict.reduce_sum()</code>. The code using the specialized reduction is very compact:</p> <pre><code>def thread_specialized(atomic_dict, iterations):\n    data = make_data(iterations)\n    atomic_dict.reduce_sum(data)\n</code></pre> <p>Multithreaded summation is a solved problem. It should be easily accessible, too.</p>"},{"location":"examples/AtomicDict/reduce/#results","title":"Results","text":"<p>The results shown here have been run with a beta version of free-threading Python 3.14.</p> <pre><code>$ python -VV\nPython 3.14.0b2 experimental free-threading build (main, Jun 13 2025, 18:57:59) [Clang 17.0.0 (clang-1700.0.13.3)]\n</code></pre> <p>Note</p> <p>Speedup is relative to the single-threaded baseline. A value below 1.0x means it performed worse than the baseline.</p>"},{"location":"examples/AtomicDict/reduce/#correctness","title":"Correctness","text":"<p>We'll go through the performance numbers soon, I promise, but first let me spend a few words about the correctness of the results.</p> <p>As you'll see from the program outputs below, all methods we've used, both based on dict and AtomicDict, have produced the expected total. For dict this really isn't big news, but I think it's worth observing how AtomicDict has made the correct result show.</p> <p>There are data races in this multithreaded operation. Multiple threads have computed their partial sum and have all concurrently tried to update the shared dictionary, racing one against the others. This is not a problem for <code>reduce()</code>: the races are transparently coped with, without explicit intervention in the code. Internally, a race has been detected and the operation retried.</p> <p>The family of <code>reduce()</code> methods effectively guarantees to observe the single-threaded result, regardless of the number of threads.</p>"},{"location":"examples/AtomicDict/reduce/#specialization","title":"Specialization","text":"<p>Using the specialized <code>reduce_sum()</code> function can yield higher performance even for single-threaded code. With multiple threads, the difference becomes increasingly more significant.</p> <pre><code>expected_total=18144000\n\nCounting keys using the built-in dict with a single thread:\n - Took 0.428s (total=18144000)\n\nCounting keys using cereggii.AtomicDict.reduce_sum():\n - Took 0.324s with 1 threads (1.3x faster, total=18144000)\n - Took 0.167s with 2 threads (2.6x faster, total=18144000)\n - Took 0.113s with 3 threads (3.8x faster, total=18144000)\n - Took 0.086s with 4 threads (5.0x faster, total=18144000)\n</code></pre>"},{"location":"examples/AtomicDict/reduce/#using-the-custom-reduce-function","title":"Using the Custom Reduce Function","text":"<p>Looking at the running time of the non-specialized function, we can observe lower  performance. It is recommended to use one of the specialized reduce functions, whenever possible.</p> <p>If you're implementing a custom function to use with <code>reduce()</code>, make sure to read the documentation about its requirements.</p> <pre><code>expected_total=18144000\n\nCounting keys using the built-in dict with a single thread:\n - Took 0.428s (total=18144000)\n\nCounting keys using cereggii.AtomicDict.reduce():\n - Took 0.453s with 1 threads (0.9x faster, total=18144000)\n - Took 0.228s with 2 threads (1.9x faster, total=18144000)\n - Took 0.153s with 3 threads (2.8x faster, total=18144000)\n - Took 0.114s with 4 threads (3.7x faster, total=18144000)\n</code></pre>"},{"location":"examples/AtomicDict/reduce/#comparison-table","title":"Comparison Table","text":"method threads time (s) speedup built-in <code>dict</code> 1 0.428 1.0x <code>AtomicDict.reduce()</code> 1 0.453 0.9x 2 0.228 1.9x 3 0.153 2.8x 4 0.114 3.7x <code>AtomicDict.reduce_sum()</code> 1 0.324 1.3x 2 0.167 2.6x 3 0.113 3.8x 4 0.086 5.0x <p>Note</p> <p>We haven't considered multithreaded scenarios for <code>dict</code> because performance would be strictly worse than the single-threaded scenario.</p>"},{"location":"examples/AtomicDict/reduce/#summary","title":"Summary","text":"<ul> <li>complying with the requirements of <code>reduce()</code> lets us write data-race-free    aggregations for <code>AtomicDict</code></li> <li><code>reduce_sum()</code> is both more convenient and more performant</li> <li>multithreading brings significant speedups when using <code>AtomicDict</code></li> </ul> <p>Refer to the <code>AtomicDict.reduce()</code> documentation for full details on writing compatible reduce functions, or check the Using <code>reduce()</code> for Averaging example for a more advanced use case.</p>"}]}